{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed8c944d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fb7373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = t.tensor(7) # s for scalar\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec87e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eecfa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the number with in a tensor, but only works with single element tensor\n",
    "s.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97157372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = t.tensor([7,7]) # v for vector\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4dcc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7075bda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb381f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = t.tensor([[7,8],\n",
    "                     [9,10]])\n",
    "M # M for matrix\n",
    "# notice convention is lowercase for scalars & vectors\n",
    "#   uppercase for matrices & tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6be032a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b9d92eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61bc6ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = t.tensor([[[1,2,3],\n",
    "                       [3,6,9],\n",
    "                       [2,4,5]]])\n",
    "T # T for tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f8e9104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa75f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice dimensions go outer to inner\n",
    "T.shape\n",
    "# so the 1 is a dummy dimension since it's not actually \"used\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c338003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  1,   2,   3],\n",
       "         [  3,   6,   9],\n",
       "         [  2,   4,   5]],\n",
       "\n",
       "        [[  1,   2,   3],\n",
       "         [  4,   5,   6],\n",
       "         [  7,   8,   9]],\n",
       "\n",
       "        [[  1,   2,   4],\n",
       "         [  8,  16,  32],\n",
       "         [ 64, 128, 256]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = t.tensor([[[1,2,3],\n",
    "                       [3,6,9],\n",
    "                       [2,4,5]],\n",
    "                     [[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]],\n",
    "                     [[1,2,4],\n",
    "                       [8,16,32],\n",
    "                       [64,128,256]]])\n",
    "R # R for rubik's cube since this one actually takes advantage of all dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b46729c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8e5ee7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape\n",
    "# so this one actually takes full advantage of each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24863e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2022, 0.5528, 0.2419, 0.6681],\n",
       "         [0.0777, 0.0615, 0.0688, 0.8312],\n",
       "         [0.5906, 0.6101, 0.7064, 0.5748]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random tensors\n",
    "rand_T = t.rand(size=(3,4))\n",
    "rand_T, rand_T.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6259b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zeros\n",
    "Z = t.zeros(size=(2,3))\n",
    "Z, Z.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "154d10ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ones\n",
    "O = t.ones(size=(4,3))\n",
    "O, O.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "811a6e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# range\n",
    "Zto10 = t.arange(0,10,1) # or specify (start=0,end=10,step=1)\n",
    "Zto10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56f121be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of zeros of the same shape as a pre-existing tensor\n",
    "ten_zeros = t.zeros_like(Zto10)\n",
    "ten_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa172aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f32T = t.tensor([3.,6.,9.],\n",
    "                         dtype=None,# the default \"None\" is actually float32\n",
    "                         device=None,# default tensor type, specify GPU\n",
    "                         requires_grad=False)# if true, operations performed on the tensor are recorded\n",
    "f32T.shape, f32T.dtype, f32T.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4497eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it on my macbook's neural engine\n",
    "x = t.ones(5,device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89f02af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now any operation happens on the GPU\n",
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c1600f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='mps', index=0), device(type='mps', index=0))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device, y.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c215952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to move a model over to mps\n",
    "#model = YourFavoriteModel()\n",
    "#model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b05ab693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), tensor([11, 12, 13]), tensor([10, 20, 30]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic operations\n",
    "test = t.tensor([1,2,3])\n",
    "test_plus = test+10\n",
    "test_mult = test*10\n",
    "test, test_plus, test_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0fa59c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([11, 12, 13]), tensor([10, 20, 30]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also use torch functions\n",
    "test_plus = t.add(test, 10)\n",
    "test_mult = t.multiply(test, 10)\n",
    "test_plus, test_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eff66597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "test*test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "555b637f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication (in this case it's just a dot product obvi)\n",
    "test @ test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19e06707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4138, 0.3141, 0.7638],\n",
       "         [0.3819, 0.4269, 0.7309],\n",
       "         [0.1975, 0.7735, 0.0335]]),\n",
       " tensor([[0.4420, 0.8549, 0.5712],\n",
       "         [0.4654, 0.8676, 0.6282],\n",
       "         [0.3837, 0.4182, 0.7174]]),\n",
       " tensor([[0.4420, 0.8549, 0.5712],\n",
       "         [0.4654, 0.8676, 0.6282],\n",
       "         [0.3837, 0.4182, 0.7174]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the torch funciton alternative\n",
    "tensor = t.rand((3,3))\n",
    "tensor, t.matmul(tensor,tensor), t.mm(tensor,tensor) # mm is shorthand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23906455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 606 µs, sys: 1.58 ms, total: 2.18 ms\n",
      "Wall time: 1.45 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  9],\n",
       "        [ 9, 36, 81],\n",
       "        [ 4, 16, 25]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# If you want to time something this is useful\n",
    "# purposely slow code\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "  value += tensor[i] * tensor[i]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29c65cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 280 µs, sys: 454 µs, total: 734 µs\n",
      "Wall time: 475 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 13,  26,  36],\n",
       "         [ 39,  78, 108],\n",
       "         [ 24,  48,  67]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# faster\n",
    "t.matmul(tensor,tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1d1c4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4765, 0.9615, 0.7560, 0.3669, 0.5726],\n",
       "         [0.5232, 0.9302, 0.2960, 0.7707, 0.4632]]),\n",
       " tensor([[0.4765, 0.5232],\n",
       "         [0.9615, 0.9302],\n",
       "         [0.7560, 0.2960],\n",
       "         [0.3669, 0.7707],\n",
       "         [0.5726, 0.4632]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose\n",
    "skinny = t.rand((2,5))\n",
    "skinny, skinny.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "888bcb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[ 2.2595,  1.2380, -0.1997,  0.6665, -0.7400,  0.7964,  0.4267,  0.6104],\n",
      "        [ 4.5145,  2.2058, -0.2241,  0.8086, -0.5308,  2.2903,  1.6631,  1.0926],\n",
      "        [ 6.7696,  3.1736, -0.2486,  0.9506, -0.3216,  3.7842,  2.8995,  1.5748]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "# to make it reproducible\n",
    "t.manual_seed(42)\n",
    "\n",
    "x = t.tensor([[1, 2],\n",
    "                    [3, 4],\n",
    "                    [5, 6]], dtype=t.float32)\n",
    "\n",
    "linear = t.nn.Linear(in_features=2,# set the inner dimension of input so you can multiply\n",
    "                        out_features=6)# describes outer value\n",
    "\n",
    "output = linear(x)\n",
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16165efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(90), tensor(45.), tensor(450))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,100,10)\n",
    "x.min(), x.max(), x.type(t.float32).mean(), x.sum()\n",
    "# notice mean is kinda weird in that you need to specify type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff5edafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(90), tensor(45.), tensor(450))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different ways to do things\n",
    "t.min(x), t.max(x), t.mean(x.type(t.float32)), t.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4eed88e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(0), tensor(9), tensor(90))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find the index where the max or min value occurs\n",
    "x.argmin(), x[x.argmin()], x.argmax(), x[x.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2e8900a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping, adding dimension, stacking, etc\n",
    "x = t.arange(1.,8.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aa8dd35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# addign extra dimension\n",
    "x_reshaped = x.reshape(1,7)\n",
    "x_reshaped, x_reshaped.shape\n",
    "# so i suppose either it does this intelligently or the first wraps the second by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4966e3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can alternatively change the view which keeps same data\n",
    "# see more https://stackoverflow.com/a/54507446/7900723\n",
    "z = x.view(1,7)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "760ed164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this new view tensor will always *share* its data with the original\n",
    "x = x+1\n",
    "z, z.shape\n",
    "# ok not sure why this isn't working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38932aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([2., 3., 4., 5., 6., 7., 8.]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing z changes x?\n",
    "z[:, 0] = 5\n",
    "z, x\n",
    "# very weird so i can't figure out which direction mutibility is supposed to go in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92542ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 3., 4., 5., 6., 7., 8.],\n",
       "         [2., 3., 4., 5., 6., 7., 8.],\n",
       "         [2., 3., 4., 5., 6., 7., 8.],\n",
       "         [2., 3., 4., 5., 6., 7., 8.]]),\n",
       " tensor([[2., 2., 2., 2.],\n",
       "         [3., 3., 3., 3.],\n",
       "         [4., 4., 4., 4.],\n",
       "         [5., 5., 5., 5.],\n",
       "         [6., 6., 6., 6.],\n",
       "         [7., 7., 7., 7.],\n",
       "         [8., 8., 8., 8.]]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stack tensors on top of each other\n",
    "x_stacked_v = t.stack([x,x,x,x], dim=0) # pay attention to dimension\n",
    "x_stacked_h = t.stack([x,x,x,x], dim=1)\n",
    "x_stacked_v, x_stacked_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d7f08f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5832, 0.3376, 0.8090],\n",
       "          [0.5779, 0.9040, 0.5547],\n",
       "          [0.3423, 0.6343, 0.3644]]]),\n",
       " torch.Size([1, 3, 3]),\n",
       " tensor([[0.5832, 0.3376, 0.8090],\n",
       "         [0.5779, 0.9040, 0.5547],\n",
       "         [0.3423, 0.6343, 0.3644]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to remove any single dummy dimension\n",
    "a = t.rand((1,3,3))\n",
    "a, a.shape, a.squeeze(), a.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e172a33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5832, 0.3376, 0.8090],\n",
       "         [0.5779, 0.9040, 0.5547],\n",
       "         [0.3423, 0.6343, 0.3644]]),\n",
       " tensor([[[0.5832, 0.3376, 0.8090],\n",
       "          [0.5779, 0.9040, 0.5547],\n",
       "          [0.3423, 0.6343, 0.3644]]]),\n",
       " tensor([[[0.5832, 0.3376, 0.8090]],\n",
       " \n",
       "         [[0.5779, 0.9040, 0.5547]],\n",
       " \n",
       "         [[0.3423, 0.6343, 0.3644]]]),\n",
       " tensor([[[0.5832],\n",
       "          [0.3376],\n",
       "          [0.8090]],\n",
       " \n",
       "         [[0.5779],\n",
       "          [0.9040],\n",
       "          [0.5547]],\n",
       " \n",
       "         [[0.3423],\n",
       "          [0.6343],\n",
       "          [0.3644]]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to add a dummy dimension at a specific index\n",
    "b = a.squeeze()\n",
    "\n",
    "b, b.unsqueeze(0), b.unsqueeze(1), b.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ac826138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), torch.Size([4, 2, 3]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can rearrange order of axes values\n",
    "x = t.rand((2,3,4))\n",
    "\n",
    "# sends 0->1, 1->2, 2->0\n",
    "x_permuted = x.permute((2,0,1)) \n",
    "\n",
    "x.shape, x_permuted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "24edb1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8879, 0.5310, 0.0187, 0.6942],\n",
       "          [0.3533, 0.0934, 0.5302, 0.1672],\n",
       "          [0.6196, 0.1803, 0.6240, 0.0419]],\n",
       " \n",
       "         [[0.7111, 0.9300, 0.5697, 0.6051],\n",
       "          [0.9739, 0.7306, 0.6755, 0.1768],\n",
       "          [0.9718, 0.2475, 0.1994, 0.5296]]]),\n",
       " tensor([[[0.8879, 0.3533, 0.6196],\n",
       "          [0.7111, 0.9739, 0.9718]],\n",
       " \n",
       "         [[0.5310, 0.0934, 0.1803],\n",
       "          [0.9300, 0.7306, 0.2475]],\n",
       " \n",
       "         [[0.0187, 0.5302, 0.6240],\n",
       "          [0.5697, 0.6755, 0.1994]],\n",
       " \n",
       "         [[0.6942, 0.1672, 0.0419],\n",
       "          [0.6051, 0.1768, 0.5296]]]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x_permuted\n",
    "# .5310 should be in\n",
    "# first entry of second dimension\n",
    "# first entry of third dimension\n",
    "# second entry of first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bb87dbbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " torch.Size([3, 2]),\n",
       " tensor([[0.4669, 0.1985, 0.4316],\n",
       "         [0.0238, 0.3256, 0.5471]]),\n",
       " tensor([[0.4669, 0.0238],\n",
       "         [0.1985, 0.3256],\n",
       "         [0.4316, 0.5471]]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try and actually understand this\n",
    "z = t.rand((2,3))\n",
    "z_permuted = z.permute((1,0))\n",
    "\n",
    "z.shape, z_permuted.shape, z, z_permuted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24a19f",
   "metadata": {},
   "source": [
    "^ I guess the concept of transpose really only makes sense in matrices, and permutation is the more general phenomenon in higher dimensions.\n",
    "\n",
    "so the \"top left\" corner and \"bottom right\" corner always stay the same\n",
    "\n",
    "here it is I've got it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f59946e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t.tensor([[[1,2,3,4],\n",
    "     [5,6,7,8],\n",
    "     [9,10,11,12]],\n",
    "    [[13,14,15,16],\n",
    "    [17,18,19,20],\n",
    "    [21,22,23,24]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97e28288",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_permuted = x.permute((1,2,0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b075f",
   "metadata": {},
   "source": [
    "So take 10 for example.\n",
    "10 is in \n",
    "- the first entry of the 0th dimension\n",
    "- the third entry of the 1st dimension\n",
    "- the second entry of the 2nd dimension\n",
    "\n",
    "and we just mapped\n",
    "- 0 -> 2\n",
    "- 1 -> 0\n",
    "- 2 -> 1\n",
    "\n",
    "so 10 should be in\n",
    "- the first entry of the 2nd dimension\n",
    "- the third entry of the 0th dimension\n",
    "- the second entry of the 1st dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "25995fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1, 13],\n",
       "         [ 2, 14],\n",
       "         [ 3, 15],\n",
       "         [ 4, 16]],\n",
       "\n",
       "        [[ 5, 17],\n",
       "         [ 6, 18],\n",
       "         [ 7, 19],\n",
       "         [ 8, 20]],\n",
       "\n",
       "        [[ 9, 21],\n",
       "         [10, 22],\n",
       "         [11, 23],\n",
       "         [12, 24]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_permuted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f96b8a",
   "metadata": {},
   "source": [
    "badabing badaboom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "02024efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(1,10).reshape(1,3,3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcfeb13",
   "metadata": {},
   "source": [
    "so i presume this reshape thing works off the mapping i just figured out basically indexing everything and using that to figure out where they should go. definitely goes outer dimension -> inner dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0560da88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First square bracket:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Second square bracket: tensor([1, 2, 3])\n",
      "Third square bracket: 1\n"
     ]
    }
   ],
   "source": [
    "# indexing\n",
    "print(f\"First square bracket:\\n{x[0]}\") \n",
    "print(f\"Second square bracket: {x[0][0]}\") \n",
    "print(f\"Third square bracket: {x[0][0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b3f02a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of 0th dimension and the 0 index of 1st dimension\n",
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "867a1d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n",
    "x[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e92874a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d2e3013b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \n",
    "x[0, 0, :] # same as x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b2df85a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing from numpy\n",
    "import numpy as np\n",
    "array = np.arange(1.,8.)\n",
    "tensor = t.from_numpy(array)\n",
    "array, tensor\n",
    "# numpy arrays turned to float64 by default, \n",
    "# but float32 is more common for torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d642a2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the reverse direction\n",
    "tensor = t.ones(7)\n",
    "numpy_tensor = tensor.numpy()\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "afd6f36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking advantage of seeds for reproducibility\n",
    "import random as r\n",
    "\n",
    "seed = 69\n",
    "t.manual_seed(seed)\n",
    "A = t.rand(3,4)\n",
    "\n",
    "# you need to reset the seed EVERY time a new tensor is called\n",
    "t.manual_seed(seed)\n",
    "B = t.rand(3,4)\n",
    "\n",
    "A == B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "357d6eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# checking for GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    \n",
    "# output should show \"tensor([1.], device='mps:0')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "46bfef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "# putting tensor on gpu\n",
    "tensor = t.tensor([1,2,3])\n",
    "print(tensor.device)\n",
    "\n",
    "tensor_on_gpu = tensor.to(\"mps\")\n",
    "print(tensor_on_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56fed2d",
   "metadata": {},
   "source": [
    "if a tensor is on GPU you cant send to numpy so u have to change it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b3c767e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585b78c",
   "metadata": {},
   "source": [
    "^ not sure if that's actually a thing for mac given that apple silicon has shared memory but i'm just following the guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb789bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
